{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# self attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class muti_head_attention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    def forward(self, X, mask=None):\n",
    "        batch_size, seuqence_length, d_model = X.size()\n",
    "        # (batch_size, seuqence_length, d_model) -> (batch_size, seuqence_length, num_heads, d_k) -> (batch_size, num_heads, seuqence_length, d_k)\n",
    "        K = self.k_linear(X)\n",
    "        Q = self.q_linear(X)\n",
    "        V = self.v_linear(X)\n",
    "        k_state = K.view(batch_size, seuqence_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        q_state = Q.view(batch_size, seuqence_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v_state = V.view(batch_size, seuqence_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # (batch_size, num_heads, sequence_length, sequence_length)\n",
    "        attention_weight = q_state @ k_state.transpose(1, 2) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                mask == 0,\n",
    "                float(\"inf\")\n",
    "            )\n",
    "        attention_weight = torch.softmax(\n",
    "            attention_weight,\n",
    "            dim = -1 # 在最后一个维度上进行softmax\n",
    "        )\n",
    "        attention_weight = self.dropout(attention_weight)\n",
    "        # (batch_size, num_heads, sequence_length, d_k)\n",
    "        attention_value = attention_weight @ v_state\n",
    "        # (batch_size, num_heads, sequence_length, d_k) -> (batch_size, sequence_length, num_heads, d_k) -> (batch_size, sequence_length, d_model)\n",
    "        attention_value = attention_value.transpose(1,2).contiguous()\n",
    "        attention_value = attention_value.view(batch_size, seuqence_length, -1)\n",
    "        output = self.out(attention_value)\n",
    "        return output\n",
    "class feed_forward(nn.Module): # 前馈神经网络\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, X):\n",
    "        X = torch.relu(self.linear1(X))\n",
    "        X = self.dropout(X)\n",
    "        X = self.linear2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# muti-head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import math\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.head_dim = d_model // heads\n",
    "        self.Q_projetion = nn.Linear(d_model, d_model) #(d_mode, head_dim * heads)\n",
    "        self.K_projetion = nn.Linear(d_model, d_model)\n",
    "        self.V_projetion = nn.Linear(d_model, d_model)\n",
    "        self.out_projetion = nn.Linear(d_model, d_model)\n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, X, attention_mask = None):\n",
    "        # shape (batch_size, seq_len, d_model)\n",
    "        batch, seq , _ = X.size()\n",
    "        Q = self.Q_projetion(X)\n",
    "        K = self.K_projetion(X)\n",
    "        V = self.V_projetion(X)\n",
    "\n",
    "        # (b, s, d) ==> (b, heads, s, head_dim) # d = heads * head_dim \n",
    "        q_state = Q.view(batch, seq, self.heads, self.head_dim).transpose(1,2)\n",
    "        k_state = K.view(batch, seq, self.heads, self.head_dim).transpose(1,2)\n",
    "        v_state = V.view(batch, seq, self.heads, self.head_dim).transpose(1,2)\n",
    "\n",
    "        # shape (b, s, s)\n",
    "        # (b, heads, s, head_dim) ==> (b,  head_dim, s)\n",
    "        # shape (b, heads,s, s)\n",
    "        attention_weight = q_state @ k_state.transpose(-1, -2) / math.sqrt(self.head_dim)\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        print(attention_weight.shape)\n",
    "        attention_weight = torch.softmax(\n",
    "            attention_weight,\n",
    "            dim = -1\n",
    "        )\n",
    "\n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        # (b, heads, s, head_dim)\n",
    "        attention_value = attention_weight @ v_state\n",
    "        out = attention_value.transpose(1, 2).contiguous()\n",
    "        out = out.view(batch, seq, -1)\n",
    "        out = self.out_projetion\n",
    "        return out\n",
    "\n",
    "        return attention_value\n",
    "X = torch.rand(3, 4, 2)\n",
    "mask = torch.tensor(\n",
    "    [\n",
    "        [1,1,1,0],\n",
    "        [1,1,0,0],\n",
    "        [1,0,0,0]\n",
    "    ]\n",
    ")\n",
    "mask = mask.unsqueeze(1).repeat(1,4,1)\n",
    "net = SelfAttention(2,1)\n",
    "net(X, attention_mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mutiheadattention(nn.Module):\n",
    "    def __init__(self, d_model, heads, gropout_rate):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.head_dim = d_model // heads\n",
    "\n",
    "        self.Q_proj = nn.Linear(d_model, d_model)\n",
    "        self.K_proj = nn.Linear(d_model, d_model)\n",
    "        self.V_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attention_dropout = nn.Dropout(gropout_rate)\n",
    "    def forward(self, X, attention_mask = None):\n",
    "        # (batch, seq, d_model)\n",
    "        batch, seq, _ = X.size()\n",
    "        Q = self.Q_proj(X)\n",
    "        K = self.K_proj(X)\n",
    "        V = self.V_proj(X)\n",
    "        out = self.out_proj(X)\n",
    "\n",
    "        # (batch, seq, d_model) ==> (b, heads, s, head_dim)\n",
    "        q_state = Q.view(batch, seq, self.heads, self.head_dim).transpose(1,2)\n",
    "        k_state = K.view(batch, seq, self.heads, self.head_dim).transpose(1,2)\n",
    "        v_state = V.view(batch, seq, self.heads, self.head_dim).transpose(1,2)\n",
    "        # (b, heads, s, s)\n",
    "        attention_weight = q_state @ v_state.transpose(-1, -2) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        \n",
    "        attention_weight = torch.softmax(\n",
    "            attention_weight, \n",
    "            dim = -1\n",
    "        )\n",
    "\n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "\n",
    "        attention_value = attention_weight @ v_state\n",
    "        # (b, heads, s, head_dim) ==> (batch, seq, heads, head_dim)\n",
    "        out = attention_value.transpose(1,2).contiguous()\n",
    "        # (batch, seq, heads, head_dim) ==> (batch, seq, d_model)\n",
    "        out = out.view(batch, seq, -1)\n",
    "        out = self.out_proj\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import math\n",
    "class Mutiattention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = self.d_model // self.heads\n",
    "        self.Q_proj = nn.Linear(d_model, d_model2) # (d_model, heads * head_dim)\n",
    "        self.K_proj = nn.Linear(d_model, d_model2) # (d_model, heads * head_dim)\n",
    "        self.V_proj = nn.Linear(d_model, d_model2) # (d_model, heads * head_dim)\n",
    "        self.out_proj = nn.Linear(d_model2, d_model) # (d_model, heads * head_dim)\n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, X, attention_mask = None):\n",
    "        # shape (batch_size, seq_len, d_model)\n",
    "        batch, seq, _ = X.size()\n",
    "        Q = self.Q_proj(X)\n",
    "        K = self.K_proj(X)\n",
    "        V = self.V_proj(X)\n",
    "\n",
    "        # shape (b, heads, seq, head_dim)\n",
    "        q_state = Q.view(batch, seq, self.heads, self.head_dim).transpose(1,2)\n",
    "        k_state = Q.view(batch, seq, self.heads, self.head_dim).transpose(1,2)\n",
    "        v_state = Q.view(batch, seq, self.heads, self.head_dim).transpose(1,2)\n",
    "        # shape(b , heads, seq, seq)\n",
    "        attention_weight = q_state @ k_state.transpose(-2,-1) // math.sqrt(self.head_dim)\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        attention_weight = torch.softmax(\n",
    "            attention_weight, dim = -1\n",
    "        )\n",
    "\n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        # shape(batch, heads, seq, head_dim)\n",
    "        attention_value = attention_weight @ v_state\n",
    "        # shape(b, s, heads, head_dim)\n",
    "        out = attention_value.tranpose(1,2),contiguous()\n",
    "        out = out.view(batch, seq, -1)\n",
    "        output = self.out_proj(out)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class Group_attention(nn.Module):\n",
    "    def __init__(self, heads, d_model,numers_key_value, dropout_rate):\n",
    "        super().__init__()\n",
    "        assert d_model % heads == 0\n",
    "        assert heads % numers_key_value == 0\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.head_dim = self.d_model // heads\n",
    "        self.numers_key_value = numers_key_value #self.heads // 4\n",
    "\n",
    "        self.Q_proj = nn.Linear(d_model, heads * self.head_dim) \n",
    "        self.K_proj = nn.Linear(d_model, numers_key_value * self.head_dim) \n",
    "        self.V_proj = nn.Linear(d_model, numers_key_value * self.head_dim) \n",
    "        self.out_proj = nn.Linear(d_model, d_model) \n",
    "\n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "    def forward(self, X, attention_mask = None):\n",
    "        # X shape(batch, seq, d_model)\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "        Q = self.Q_proj(X)\n",
    "        K = self.K_proj(X)\n",
    "        V = self.V_proj(X)\n",
    "\n",
    "        # atttetntion_weight (batch, heads, seq_q, seq_k)\n",
    "        q = Q.view(batch_size, seq_len, self.heads, self.head_dim).transpose(1,2)\n",
    "        k = K.view(batch_size, seq_len, self.numers_key_value, self.head_dim).transpose(1,2)\n",
    "        v = V.view(batch_size, seq_len, self.numers_key_value, self.head_dim).transpose(1,2)\n",
    "        # k, v 需要repeat。torch 存在广播操作。\n",
    "        k = k.repeat_interleave(self.heads // self.numers_key_value, dim = 1)\n",
    "        v = v.repeat_interleave(self.heads // self.numers_key_value, dim = 1)\n",
    "\n",
    "        attention_weight = q @ k.transpose(-1,-2) / math.sqrt(self.head_dim)\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        attention_weight = torch.softmax(attention_weight, dim = -1)\n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        output = attention_weight @ v\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math \n",
    "class attention(nn.Module):\n",
    "    def __init__(self, heads, hidden_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % heads = 0\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = hidden_dim // heads\n",
    "        self.Q = nn.Linear(hidden_dim, hidden_dim) # (hidden_dim, head * head_dim)\n",
    "        self.K = nn.Linear(hidden_dim, hidden_dim) # (hidden_dim, head * head_dim)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim) # (hidden_dim, head * head_dim)\n",
    "        self.O = nn.Linear(hidden_dim, hidden_dim) # (hidden_dim, head * head_dim)\n",
    "        self.attention_dropout = dropout_rate\n",
    "    def forward(self, X, attention_mask = None):\n",
    "        #X : (batch_size, seq_len, hiddem_dim)\n",
    "        batch_size, seq_len, hiddem_dim = X.size()\n",
    "        Q = self.Q(X)\n",
    "        K = self.K(X)\n",
    "        V = self.V(X)\n",
    "        #分割头\n",
    "        #(batch, heads, seq, head_dim)\n",
    "        q_state = Q.view(batch_size, seq_len, self.heads, self.head_dim).shape(1,2)\n",
    "        k_state = K.view(batch_size, seq_len, self.heads, self.head_dim).shape(1,2)\n",
    "        v_state = V.view(batch_size, seq_len, self.heads, self.head_dim).shape(1,2)\n",
    "        #(b,hs,s,s)\n",
    "        attention_weight = q_state @ k_state.tranpose(-1,-2) // math.sqrt(self.head_dim)\n",
    "        if attention_mask:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask = 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        attention_weight = torch.softmax(\n",
    "            attention_weight,\n",
    "            dim = -1\n",
    "        )\n",
    "        attention_weight = self.attention_dropout(\n",
    "            attention_weight\n",
    "        )\n",
    "        #(b,hs, s, head_dim)\n",
    "        attention_value = attention_weight @ v_state\n",
    "        #(b, s, heads, head_dim)\n",
    "        attention_value.tranpose(1,2),contiguous()\n",
    "        out = out.view(batch_size, seq_len, -1)\n",
    "        output = self.O(out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import math\n",
    "class Mutiattention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = self.d_model // self.heads\n",
    "        self.Q_proj = nn.Linear(d_model, d_model2) # (d_model, heads * head_dim)\n",
    "        self.K_proj = nn.Linear(d_model, d_model2) # (d_model, heads * head_dim)\n",
    "        self.V_proj = nn.Linear(d_model, d_model2) # (d_model, heads * head_dim)\n",
    "        self.out_proj = nn.Linear(d_model2, d_model) # (d_model, heads * head_dim)\n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, X, attention_mask = None):\n",
    "        # shape (batch_size, seq_len, d_model)\n",
    "        batch, seq, _ = X.size()\n",
    "        Q = self.Q_proj(X)\n",
    "        K = self.K_proj(X)\n",
    "        V = self.V_proj(X)\n",
    "\n",
    "        # shape (b, heads, seq, head_dim)\n",
    "        q_state = Q.view(batch, seq, self.heads, self.head_dim).transpose(1,2)\n",
    "        k_state = Q.view(batch, seq, self.heads, self.head_dim).transpose(1,2)\n",
    "        v_state = Q.view(batch, seq, self.heads, self.head_dim).transpose(1,2)\n",
    "        # shape(b , heads, seq, seq)\n",
    "        attention_weight = q_state @ k_state.transpose(-2,-1) // math.sqrt(self.head_dim)\n",
    "        if attention_mask is not None:\n",
    "            attention_weight = attention_weight.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "        attention_weight = torch.softmax(\n",
    "            attention_weight, dim = -1\n",
    "        )\n",
    "\n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        # shape(batch, heads, seq, head_dim)\n",
    "        attention_value = attention_weight @ v_state\n",
    "        # shape(b, s, heads, head_dim)\n",
    "        out = attention_value.tranpose(1,2),contiguous()\n",
    "        out = out.view(batch, seq, -1)\n",
    "        output = self.out_proj(out)\n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
