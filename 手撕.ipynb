{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096] \n",
      "\n",
      "[[0.00548473 0.01490905 0.04052699]\n",
      " [0.01490905 0.11016379 0.8140064 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def safe_softmax(x):\n",
    "    max_x = np.max(x, axis = -1, keepdims=True)\n",
    "    exp_x = np.exp(x - max_x)\n",
    "    sum_exp_x = np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "    return exp_x / sum_exp_x\n",
    "# 一维输入测试\n",
    "x = np.array([1, 2, 3])\n",
    "print(softmax(x), '\\n')  # 输出 ≈ [0.0900, 0.2447, 0.6652]\n",
    "\n",
    "# 二维输入测试\n",
    "x = np.array([[1, 2, 3],\n",
    "              [2, 4, 6]])\n",
    "print(softmax(x))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m K \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]]])\n\u001b[0;32m     12\u001b[0m V \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]]])\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# 输出 [[[2.5, 2.0, 3.5]]]\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m, in \u001b[0;36mattention\u001b[1;34m(Q, K, V)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattention\u001b[39m(Q, K, V):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Q, K, V 的维度 shape 为 (batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# 计算 Q 和 K 的转置\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     attention_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(Q, \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(Q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      6\u001b[0m     attention_score \u001b[38;5;241m=\u001b[39m safe_softmax(attention_score)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# 计算加权和\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "def attention(Q, K, V):\n",
    "    # Q, K, V 的维度 shape 为 (batch_size, seq_len, d_model)\n",
    "    # 计算 Q 和 K 的转置\n",
    "    attention_score = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(Q.shape[-1])\n",
    "\n",
    "    attention_score = safe_softmax(attention_score)\n",
    "    # 计算加权和\n",
    "    attention_output = np.matmul(attention_score, V)\n",
    "    return attention_output\n",
    "Q = np.array([[[1, 2, 3], [4, 5, 6]]])\n",
    "K = np.array([[[1, 0, 0], [0, 1, 0]]])\n",
    "V = np.array([[[0, 1, 0], [1, 0, 1]]])\n",
    "print(attention(Q, K, V))  # 输出 [[[2.5, 2.0, 3.5]]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
