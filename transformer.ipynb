{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从零开始的attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入需要的库和函数\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n层的克隆\n",
    "\n",
    "gpt 编码器和解码器需要叠加12层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_module_to_modulelist(module, module_num):\n",
    "    \"\"\"\n",
    "    克隆n个Module类放入ModuleList中，并返回ModuleList，这个ModuleList中的每个Module都是一模一样的\n",
    "    nn.ModuleList，它是一个储存不同 module，并自动将每个 module 的 parameters 添加到网络之中的容器。\n",
    "    你可以把任意 nn.Module 的子类 (比如 nn.Conv2d, nn.Linear 之类的) 加到这个 list 里面，\n",
    "    加入到 nn.ModuleList 里面的 module 是会自动注册到整个网络上的，\n",
    "    同时 module 的 parameters 也会自动添加到整个网络中。\n",
    "    :param module: 被克隆的module\n",
    "    :param module_num: 被克隆的module数\n",
    "    :return: 装有module_num个相同module的ModuleList\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([deepcopy(module) for _ in range(module_num)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 层归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    :param x_size: 特征的维度\n",
    "    :param eps: eps是一个平滑的过程，取值通常在（10^-4~10^-8 之间）\n",
    "    其含义是，对于每个参数，随着其更新的总距离增多，其学习速率也随之变慢。\n",
    "    防止出现除以0的情况。\n",
    "\n",
    "    nn.Parameter将一个不可训练的类型Tensor转换成可以训练的类型parameter，\n",
    "    并将这个parameter绑定到这个module里面。\n",
    "    使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。\n",
    "    \"\"\"\n",
    "    def __init__(self, x_size, eps = 1e-6):\n",
    "        super().__init__()\n",
    "        self.x_size = x_size\n",
    "        self.eps = eps\n",
    "        self.alapha = nn.Parameter(torch.ones(self.x_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.x_size))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # keepdim=True 是一个关键参数，其作用是保持结果张量的维度数量与输入张量 X 相同。\n",
    "        mean = X.mean(-1, keepdim=True) # 求均值\n",
    "        std = X.std(-1, keepdim=True)  # 求标准差\n",
    "        norm = self.alpha * (X - mean) / (std + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    把向量构造成d_model维度的词向量，以便后续送入编码器\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        :param vocab_size: 字典长度\n",
    "        :param d_model: 词向量维度\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # 字典中有vocab_size个词，词向量维度是d_model，每个词将会被映射成d_model维度的向量\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.embedding(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 位置编码\n",
    "https://0809zheng.github.io/2022/07/01/posencode.html \n",
    "\n",
    "绝对位置编码 ==> \n",
    "1. 三角位置编码\n",
    "\n",
    "     $$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}})$$\n",
    "\n",
    "    $$PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})$$   \n",
    "\n",
    "\n",
    "其中，pos表示单词所在的位置。2i和2i+1表示位置编码向量中的对应维度。d则为总维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    正弦位置编码，即通过三角函数构建位置编码\n",
    "\n",
    "    Implementation based on \"Attention Is All You Need\"\n",
    "    :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout, max_seq_len = 5000):\n",
    "        \"\"\"\n",
    "        :param d_mode: 位置向量的向量维度，一般与词向量维度相同，即d_model\n",
    "        :param dropout: Dropout层的比率\n",
    "        :param max_len: 句子的最大长度\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.drop_out = nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "        # 判断能够构建位置向量\n",
    "        if d_model % 2 != 0:\n",
    "            raise ValueError(f\"不能使用 sin/cos 位置编码，得到了奇数的维度{d_model:d}，应该使用偶数维度\")\n",
    "        \n",
    "        \"\"\"\n",
    "        构建位置编码pe\n",
    "        pe公式为：\n",
    "        PE(pos,2i/2i+1) = sin/cos(pos/10000^{2i/d_{model}})\n",
    "        \"\"\"\n",
    "        pe = torch.zeros(max_seq_len, d_model) # 初始化pe\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** (i / d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** (i / d_model)))\n",
    "        # 在最前面添加一个批次维度，\n",
    "        # 形状从 [max_seq_len, d_model] 变成 [1, max_seq_len, d_model]\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        # register_buffer: 将 pe 注册为一个不可训练的张量（不会计算梯度）\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        词向量和位置编码拼接并输出\n",
    "        :param X: 词向量序列（FloatTensor），``(seq_len, batch_size, self.dim)``\n",
    "        :return: 词向量和位置编码的拼接\n",
    "        \"\"\"\n",
    "        X = X * math.sqrt(self.d_model)\n",
    "        seq_len = X.size(1)\n",
    "        X = X + self.pe[:, :seq_len]\n",
    "        X = self.drop_out(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. 为什么需要 `X * math.sqrt(self.d_model)`？**\n",
    "\n",
    "#### **背景**\n",
    "Transformer 模型中的输入 `X` 通常是通过嵌入层（embedding layer）生成的，维度是 `[batch_size, seq_len, d_model]`。这些嵌入的值通常是从随机初始化开始训练的，数值范围相对较小。\n",
    "\n",
    "#### **作用**\n",
    "1. **数值稳定性**：\n",
    "   - 嵌入层生成的 `X` 数值较小，而位置编码（`pe`）的值范围可能更大（例如，包含 `sin` 和 `cos` 函数的结果）。\n",
    "   - 通过乘以 `math.sqrt(d_model)`，让 `X` 和 `pe` 的数值范围更匹配，避免加法时信息失衡。\n",
    "\n",
    "2. **缩放输入**：\n",
    "   - 缩放可以防止在模型训练初期，注意力分数（dot product attention）过小或过大，从而影响梯度流。\n",
    "\n",
    "#### **如果不做会怎样？**\n",
    "- 如果不做 `X * math.sqrt(d_model)`，数值范围失衡可能导致位置编码对输入信号的干扰过大，影响训练效果。\n",
    "- 但在很多实现中，也可以省略这一步，实际效果因具体任务而异。\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 这一步是否需要 Dropout？如果需要，放在哪里？**\n",
    "\n",
    "#### **是否需要 Dropout**\n",
    "- **需要 Dropout** 的场景：\n",
    "  - 如果位置编码的加法操作直接影响模型的学习稳定性，可以在加入位置编码后进行 Dropout，以缓解过拟合。\n",
    "- **不需要 Dropout** 的场景：\n",
    "  - 如果模型有其他地方的正则化（如嵌入层、注意力层或前馈网络中使用 Dropout），可以不在这里再添加 Dropout。\n",
    "\n",
    "#### **Dropout 的位置**\n",
    "如果需要 Dropout，应该放在位置编码加法之后：\n",
    "```python\n",
    "X = X + self.pe[:, :seq_len]\n",
    "X = self.dropout(X)\n",
    "```\n",
    "这是因为位置编码只需要加入一次，Dropout 应该作用在最终的结合结果上。\n",
    "\n",
    "#### **为什么可能不需要 Dropout**\n",
    "- 位置编码本身是一个固定的常量，并不会参与训练。如果模型的主要学习部分（如注意力层和前馈网络）已经加了足够的 Dropout，这里再加可能是多余的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自注意力计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(query, key, value, dropout = None, mask = None):\n",
    "    \"\"\"\n",
    "    自注意力计算\n",
    "    :param query: Q\n",
    "    :param key: K\n",
    "    :param value: V\n",
    "    :param dropout: drop比率\n",
    "    :param mask: 是否mask\n",
    "    :return: 经自注意力机制计算后的值\n",
    "    \"\"\"\n",
    "    d_k = key.size(-1) # 防止softmax未来求梯度消失时的d_k\n",
    "    # Q,K相似度计算公式：\\frac{Q^TK}{\\sqrt{d_k}}\n",
    "    attention_weight = query @ key.transpose(-1, -2) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        \"\"\"\n",
    "        attention_weight.masked_fill默认是按照传入的mask中为1的元素所在的索引，\n",
    "        在attention_weight中相同的的索引处替换为value，替换值为负无穷，使得softmax中计算直接让其为0\n",
    "        \"\"\"\n",
    "        # mask.cuda()\n",
    "        # 进行mask操作，由于参数mask==0，因此替换上述mask中为0的元素所在的索引\n",
    "        attention_weight = attention_weight.masked_fill(mask == 0, float('-inf'))\n",
    "    # (batch_size, seq_q, seq_k)\n",
    "    # Q1 ==> k1, k2, k3...\n",
    "    # K1 ==> q1, q2, q3...\n",
    "    attention_weight = F.softmax(attention_weight, dim=-1)  # 进行softmax\n",
    "    if dropout is not None:\n",
    "        attention_weight = dropout(attention_weight)\n",
    "    \n",
    "    attention_value = attention_weight @ value\n",
    "    return attention_value, attention_weight "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多头注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        :param head: 头数\n",
    "        :param d_model: 词向量的维度，必须是head的整数倍\n",
    "        :param dropout: drop比率\n",
    "        \"\"\"\n",
    "        assert (d_model % head == 0)  # 确保词向量维度是头数的整数倍\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = self.d_model // self.heads\n",
    "        \"\"\"\n",
    "        由于多头注意力机制是针对多组Q、K、V，因此有了下面这四行代码，具体作用是，\n",
    "        针对未来每一次输入的Q、K、V，都给予参数进行构建\n",
    "        其中linear_out是针对多头汇总时给予的参数\n",
    "        \"\"\"\n",
    "        self.Q_proj = nn.Linear(d_model, d_model)  # 进行一个普通的全连接层变化，但不修改维度\n",
    "        self.K_proj = nn.Linear(d_model, d_model)\n",
    "        self.V_proj = nn.Linear(d_model, d_model)\n",
    "        self.OUT_proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            \"\"\"\n",
    "            多头注意力机制的线性变换层是4维，是把query[batch, seq_len, d_model]变成[batch, -1, heads, head_dim]\n",
    "            再1，2维交换变成[batch, heads, -1, head_dim], 所以mask要在第二维（head维）添加一维，与后面的self_attention计算维度一样\n",
    "            具体点将，就是：\n",
    "            因为mask的作用是未来传入self_attention这个函数的时候，作为masked_fill需要mask哪些信息的依据\n",
    "            针对多head的数据，Q、K、V的形状维度中，只有head是通过view计算出来的，是多余的，为了保证mask和\n",
    "            view变换之后的Q、K、V的形状一直，mask就得在head这个维度添加一个维度出来，进而做到对正确信息的mask\n",
    "            \"\"\"\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        batch_size = query.size(0)  # batch_size大小，假设query的维度是：[10, 32, 512]，其中10是batch_size的大小\n",
    "        \"\"\"\n",
    "        下列三行代码都在做类似的事情，对Q、K、V三个矩阵做处理\n",
    "        其中view函数是对Linear层的输出做一个形状的重构，其中-1是自适应（自主计算）\n",
    "        从这种重构中，可以看出，虽然增加了头数，但是数据的总维度是没有变化的，也就是说多头是对数据内部进行了一次拆分\n",
    "        transopose(1,2)是对前形状的两个维度(索引从0开始)做一个交换，例如(2,3,4,5)会变成(2,4,3,5)\n",
    "        因此通过transpose可以让view的第二维度参数变成n_head\n",
    "        假设Linear成的输出维度是：[10, 32, 512]，其中10是batch_size的大小\n",
    "        注：这里解释了为什么d_model // head == head_dim，如若不是，则view函数做形状重构的时候会出现异常\n",
    "        \"\"\"\n",
    "        query = self.Q_proj(query).view(batch_size, -1, self.head, self.head_dim).transpose(1, 2)  # [b, 8, 32, 64]，head=8\n",
    "        key = self.K_proj(key).view(batch_size, -1, self.head, self.head_dim).transpose(1, 2)  # [b, 8, 28, 64]\n",
    "        value = self.V_proj(value).view(batch_size, -1, self.head, self.head_dim).transpose(1, 2)  # [b, 8, 28, 64]\n",
    "        \n",
    "        # x是通过自注意力机制计算出来的值， self.attn_softmax是相似概率分布\n",
    "        X, self.attn_softmax = self_attention(query, key, value, dropout=self.dropout, mask=mask)\n",
    "\n",
    "        \"\"\"\n",
    "        下面的代码是汇总各个头的信息，拼接后形成一个新的x\n",
    "        其中self.head * self.head_dim，可以看出x的形状是按照head数拼接成了一个大矩阵，然后输入到linear_out层添加参数\n",
    "        contiguous()是重新开辟一块内存后存储x，然后才可以使用.view方法，否则直接使用.view方法会报错\n",
    "        \"\"\"\n",
    "        X = X.transpose(1, 2).contiguous().view(n_batch, -1, self.head * self.head_dim)\n",
    "        return self.OUT_proj(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前馈神经网络FNN\n",
    "主要是：接收注意力层的输入，并进行升高维度 - 激活函数 - 减低维度\n",
    "\n",
    "除了attention子层之外，我们的编码器和解码器中的每个层都包含一个全连接的前馈网络，该网络在每个层的位置相同（都在每个encoder-layer或者decoder-layer的最后）。该前馈网络包括两个线性变换，并在两个线性变换中间有一个ReLU激活函数。\n",
    "\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$                                                                        \n",
    "\n",
    "尽管两层都是线性变换，但它们在层与层之间使用不同的参数。另一种描述方式是两个内核大小为1的卷积。 输入和输出的维度都是 $d_{\\text{model}}=512$, 内层维度是$d_{ff}=2048$。（也就是第一层输入512维,输出2048维；第二层输入2048维，输出512维）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        '''\n",
    "        :param d_model: FFN第一层输入的维度\n",
    "        :param d_ff: FNN第二层隐藏层输入的维度，一般为4 * d_model\n",
    "        :param dropout: drop比率\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff) # 升维度\n",
    "        self.linear2 = nn.Linear(d_ff, d_model) # 降维度\n",
    "        self.ff_dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        :param x: 输入数据，形状为(batch_size, input_len, model_dim)\n",
    "        :return: 输出数据（FloatTensor），形状为(batch_size, input_len, model_dim)\n",
    "        \"\"\"\n",
    "        inter = self.ff_dropout(F.relu(self.linear1(self.layer_norm(X)))) # 升维度加激活\n",
    "        output = self.linear2(inter)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 残差链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    子层的连接: layer_norm(x + sublayer(x))\n",
    "    上述可以理解为一个残差网络加上一个LayerNorm归一化\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param size: d_model\n",
    "        :param dropout: drop比率\n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.layer_norm = LayerNorm(size)\n",
    "        # TODO：在SublayerConnection中LayerNorm可以换成nn.BatchNorm2d\n",
    "        # self.layer_norm = nn.BatchNorm2d()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return self.dropout(self.layer_norm(x + sublayer(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单一encoder层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    一层编码Encoder层\n",
    "    MultiHeadAttention -> Add & Norm -> Feed Forward -> Add & Norm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, attn, feed_forward, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param size: d_model\n",
    "        :param attn: 已经初始化的Multi-Head Attention层\n",
    "        :param feed_forward: 已经初始化的Feed Forward层\n",
    "        :param dropout: drop比率\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attn = attn\n",
    "        self.feed_forward = feed_forward\n",
    "\n",
    "        \"\"\"\n",
    "        下面一行的作用是因为一个Encoder层具有两个残差结构的网络\n",
    "        因此构建一个ModuleList存储两个SublayerConnection，以便未来对数据进行残差处理\n",
    "        \"\"\"\n",
    "        self.sublayer_connection_list = clone_module_to_modulelist(SublayerConnection(size, dropout), 2)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        :param x: Encoder层的输入\n",
    "        :param mask: mask标志\n",
    "        :return: 经过一个Encoder层处理后的输出\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        编码层第一层子层\n",
    "        self.attn 应该是一个已经初始化的Multi-Head Attention层\n",
    "        把Encoder的输入数据x和经过一个Multi-Head Attention处理后的x_attn送入第一个残差网络进行处理得到first_x\n",
    "        \"\"\"\n",
    "        first_x = self.sublayer_connection_list[0](x, lambda x_attn: self.attn(x, x, x, mask))\n",
    "\n",
    "        \"\"\"\n",
    "        编码层第二层子层\n",
    "        把经过第一层子层处理后的数据first_x与前馈神经网络送入第二个残差网络进行处理得到Encoder层的输出\n",
    "        \"\"\"\n",
    "        return self.sublayer_connection_list[1](first_x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单一decoder层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    一层解码Decoder层\n",
    "    Mask MultiHeadAttention -> Add & Norm -> Multi-Head Attention -> Add & Norm\n",
    "    -> Feed Forward -> Add & Norm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, attn, feed_forward, sublayer_num, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: d_model\n",
    "        :param attn: 已经初始化的Multi-Head Attention层\n",
    "        :param feed_forward: 已经初始化的Feed Forward层\n",
    "        :param sublayer_num: 解码器内部子层数，如果未来r2l_memory传入有值，则为4层，否则为普通的3层\n",
    "        :param dropout: drop比率\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.attn = attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer_connection_list = clone_module_to_modulelist(SublayerConnection(d_model, dropout), sublayer_num)\n",
    "\n",
    "    def forward(self, x, l2r_memory, src_mask, trg_mask, r2l_memory=None, r2l_trg_mask=None):\n",
    "        \"\"\"\n",
    "        :param x: Decoder的输入(captioning)\n",
    "        :param l2r_memory: Encoder的输出，作为Multi-Head Attention的K，V值，为从左到右的Encoder的输出\n",
    "        :param src_mask: 编码器输入的填充掩码\n",
    "        :param trg_mask: 解码器输入的填充掩码和序列掩码，即对后面单词的掩码\n",
    "        :param r2l_memory: 从右到左解码器的输出\n",
    "        :param r2l_trg_mask: 从右到左解码器的输出的填充掩码和序列掩码\n",
    "        :return: Encoder的输出\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        解码器第一层子层\n",
    "        把Decoder的输入数据x和经过一个Masked Multi-Head Attention处理后的first_x_attn送入第一个残差网络进行处理得到first_x\n",
    "        \"\"\"\n",
    "        first_x = self.sublayer_connection_list[0](x, lambda first_x_attn: self.attn(x, x, x, trg_mask))\n",
    "\n",
    "        \"\"\"\n",
    "        解码器第二层子层\n",
    "        把第一层子层得到的first_x和\n",
    "        经过一个Multi-Head Attention处理后的second_x_attn（由first_x和Encoder的输出进行自注意力计算）\n",
    "        送入第二个残差网络进行处理\n",
    "        \"\"\"\n",
    "        second_x = self.sublayer_connection_list[1](first_x,\n",
    "                                                    lambda second_x_attn: self.attn(first_x, l2r_memory, l2r_memory,\n",
    "                                                                                    src_mask))\n",
    "\n",
    "        \"\"\"\n",
    "        解码器第三层子层\n",
    "        把经过第二层子层处理后的数据second_x与前馈神经网络送入第三个残差网络进行处理得到Decoder层的输出\n",
    "        \n",
    "        如果有r2l_memory数据，则还需要经过一层多头注意力计算，也就是说会有四个残差网络\n",
    "        r2l_memory是让Decoder层多了一层双向编码中从右到左的编码层\n",
    "        而只要三个残差网络的Decoder层只有从左到右的编码\n",
    "        \"\"\"\n",
    "        if not r2l_memory:\n",
    "            # 进行从右到左的编码，增加语义信息\n",
    "            third_x = self.sublayer_connection_list[-2](second_x,\n",
    "                                                        lambda third_x_attn: self.attn(second_x, r2l_memory, r2l_memory,\n",
    "                                                                                       r2l_trg_mask))\n",
    "            return self.sublayer_connection_list[-1](third_x, self.feed_forward)\n",
    "        else:\n",
    "            return self.sublayer_connection_list[-1](second_x, self.feed_forward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
